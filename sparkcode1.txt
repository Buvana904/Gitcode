//rddparallelise

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

object rddparallelise {
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkSession.builder().master("local[1]")
      .appName("SparkByExamples.com")
      .getOrCreate()
    val rdd: RDD[Int] = spark.sparkContext.parallelize(List(1, 2, 3, 4, 5))
    val rddCollect: Array[Int] = rdd.collect()
    println("Number of Partitions: " + rdd.getNumPartitions)
    println("Action: First element: " + rdd.first())
    println("Action: RDD converted to Array[Int] : ")
    rddCollect.foreach(println)
  }
}


//foldbyexample

import org.apache.spark.sql.SparkSession

object foldexmaple extends App {
  val spark = SparkSession.builder()
    .appName("SparkByExamples.com")
    .master("local[3]")
    .getOrCreate()

  spark.sparkContext.setLogLevel("ERROR")

  //fold example
  val listRdd = spark.sparkContext.parallelize(List(1, 2, 3, 4, 5, 3, 2))
  println("Partitions : " + listRdd.getNumPartitions)
  println("Total : " + listRdd.fold(0)((acc, ele) => {
    acc + ele
  }))
  println("Total with init value 2 : " + listRdd.fold(2)((acc, ele) => {
    acc + ele
  }))
  println("Min : " + listRdd.fold(0)((acc, ele) => {
    acc min ele
  }))
  println("Max : " + listRdd.fold(0)((acc, ele) => {
    acc max ele
  }))

  val inputRDD = spark.sparkContext.parallelize(List(("Z", 1), ("A", 20), ("B", 30),
    ("C", 40), ("B", 30), ("B", 60)))

  println("Total : " + inputRDD.fold(("", 0))((acc, ele) => {
    ("Total", acc._2 + ele._2)
  }))
  println("Min : " + inputRDD.fold(("", 0))((acc, ele) => {
    ("Min", acc._2 min ele._2)
  }))
  println("Max : " + inputRDD.fold(("", 0))((acc, ele) => {
    ("Max", acc._2 max ele._2)
  }))

}


//using windowfunction/sum,max,min,avg



import org.apache.spark.sql.{SparkSession, functions}
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.{avg, col, row_number, sum}

object windowgroupbyfirst extends App {
  val spark: SparkSession = SparkSession.builder()
    .master("local[1]")
    .appName("SparkByExamples.com")
    .getOrCreate()

  spark.sparkContext.setLogLevel("ERROR")

  import spark.implicits._

  val simpleData = Seq(("James", "Sales", 3000),
    ("Michael", "Sales", 4600),
    ("Robert", "Sales", 4100),
    ("Maria", "Finance", 3000),
    ("Raman", "Finance", 3000),
    ("Scott", "Finance", 3300),
    ("Jen", "Finance", 3900),
    ("Jeff", "Marketing", 3000),
    ("Kumar", "Marketing", 2000)
  )
  val df = simpleData.toDF("employee_name", "department", "salary")
  df.show()

  //Get the first row from a group.
  val w2 = Window.partitionBy("department").orderBy(col("salary"))
  df.withColumn("row", row_number.over(w2))
    .where($"row" === 1).drop("row")
    .show()

  //Retrieve Highest salary
  val w3 = Window.partitionBy("department").orderBy(col("salary").desc)
  df.withColumn("row", row_number.over(w3))
    .where($"row" === 1).drop("row")
    .show()

  //Maximum, Minimum, Average, total salary for each window group
  val w4 = Window.partitionBy("department")
  val aggDF = df.withColumn("row", row_number.over(w3))
    .withColumn("avg", avg(col("salary")).over(w4))
    .withColumn("sum", sum(col("salary")).over(w4))
    .withColumn("min", functions.min(col("salary")).over(w4))
    .withColumn("max", functions.max(col("salary")).over(w4))
    .where(col("row") === 1).select("department", "avg", "sum", "min", "max")
    .show()

}


//Broadcast variable

import org.apache.spark.sql.SparkSession

object RDDBroadcast extends App {

  val spark = SparkSession.builder()
    .appName("SparkByExamples.com")
    .master("local")
    .getOrCreate()

  val states = Map(("NY","New York"),("CA","California"),("FL","Florida"))
  val countries = Map(("USA","United States of America"),("IN","India"))

  val broadcastStates = spark.sparkContext.broadcast(states)
  val broadcastCountries = spark.sparkContext.broadcast(countries)

  val data = Seq(("James","Smith","USA","CA"),
    ("Michael","Rose","USA","NY"),
    ("Robert","Williams","USA","CA"),
    ("Maria","Jones","USA","FL")
  )

  val rdd = spark.sparkContext.parallelize(data)

  val rdd2 = rdd.map(f=>{
    val country = f._3
    val state = f._4
    val fullCountry = broadcastCountries.value.get(country).get
    val fullState = broadcastStates.value.get(state).get
    (f._1,f._2,fullCountry,fullState)
  })

  println(rdd2.collect().mkString("\n"))

}


//partition & coalesce


import org.apache.spark.sql.SparkSession

object RDDRepartitionExample extends App {

  val spark:SparkSession = SparkSession.builder()
    .master("local[5]")
    .appName("SparkByExamples.com")
    .getOrCreate()

  val rdd = spark.sparkContext.parallelize(Range(0,20))
  println("From local[5]"+rdd.partitions.size)

  val rdd1 = spark.sparkContext.parallelize(Range(0,20), 6)
  println("parallelize : "+rdd1.partitions.size)

  rdd1.partitions.foreach(f=> f.toString)
  val rddFromFile = spark.sparkContext.textFile(""C:\Users\buvan\OneDrive\Desktop\hadoop.txt"",9)

  println("TextFile : "+rddFromFile.partitions.size)

  rdd1.saveAsTextFile("c:/tmp/partition")
  val rdd2 = rdd1.repartition(4)
  println("Repartition size : "+rdd2.partitions.size)

  rdd2.saveAsTextFile("c:/tmp/re-partition")

  val rdd3 = rdd1.coalesce(4)
  println("Repartition size : "+rdd3.partitions.size)

  rdd3.saveAsTextFile("c:/tmp/coalesce")


}

